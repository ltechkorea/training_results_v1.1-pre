+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ '[' 1 -eq 1 ']'
+ srun --ntasks=1 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
srun: Job 1417062 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Step created for job 1417062
Clearing cache on luna-0030
vm.drop_caches = 3
+ srun --ntasks=1 --container-name=language_model python -c '
import mlperf_logger
mlperf_logger.log_event(key=mlperf_logger.constants.CACHE_CLEAR, value=True)'
:::MLLOG {"namespace": "", "time_ms": 1621399980374, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
+ '[' 0 -eq 0 ']'
+ srun -l --mpi=none --ntasks=8 --ntasks-per-node=8 --container-name=language_model --container-mounts=/raid/datasets/bert/hdf5/v1p0_ref/4096_shards_uncompressed:/workspace/data,/raid/datasets/bert/hdf5/v1p0_ref/4096_shards_uncompressed:/workspace/data_phase2,/lustre/fsw/mlperf-ci/23336612/ci_checkpoints:/results,/raid/datasets/bert/checkpoints/checkpoint_phase1:/workspace/phase1,/raid/datasets/bert/hdf5/v1p0_ref/eval_uncompressed:/workspace/evaldata,/lustre/fsw/mlperf/mlperft-bert/unit_test:/workspace/unit_test_data sh -c '/workspace/bert/run_and_time.sh "    ./bind.sh --cpu=exclusive --ib=single --cluster=selene --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=56     --learning_rate=3.5e-4     --opt_lamb_beta_1=0.9     --opt_lamb_beta_2=0.999     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=7100     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.01     --max_samples_termination=4500000     --eval_iter_start_samples=150000 --eval_iter_samples=150000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding     --distributed_lamb   --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks=1      --gradient_accumulation_steps=1     --log_freq=0     --local_rank=${SLURM_LOCALID}     --bert_config_path=/workspace/phase1/bert_config.json --allreduce_post_accumulation --allreduce_post_accumulation_fp16" 7504 '
3: Run vars: id 1417062 gpus 8 mparams
6: Run vars: id 1417062 gpus 8 mparams
3: STARTING TIMING RUN AT 2021-05-18 09:53:01 PM
3: + eval '     ./bind.sh --cpu=exclusive --ib=single --cluster=selene --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=56     --learning_rate=3.5e-4     --opt_lamb_beta_1=0.9     --opt_lamb_beta_2=0.999     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=7100     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.01     --max_samples_termination=4500000     --eval_iter_start_samples=150000 --eval_iter_samples=150000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding     --distributed_lamb   --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks
3: =1      --gradient_accumulation_steps=1     --log_freq=0     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.json --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --seed=7504'
3: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=selene -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --allred
3: uce_post_accumulation --allreduce_post_accumulation_fp16 --seed=7504
1: Run vars: id 1417062 gpus 8 mparams
7: Run vars: id 1417062 gpus 8 mparams
0: Run vars: id 1417062 gpus 8 mparams
2: Run vars: id 1417062 gpus 8 mparams
4: Run vars: id 1417062 gpus 8 mparams
5: Run vars: id 1417062 gpus 8 mparams
6: STARTING TIMING RUN AT 2021-05-18 09:53:01 PM
6: + eval '     ./bind.sh --cpu=exclusive --ib=single --cluster=selene --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=56     --learning_rate=3.5e-4     --opt_lamb_beta_1=0.9     --opt_lamb_beta_2=0.999     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=7100     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.01     --max_samples_termination=4500000     --eval_iter_start_samples=150000 --eval_iter_samples=150000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding     --distributed_lamb   --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks
6: =1      --gradient_accumulation_steps=1     --log_freq=0     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.json --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --seed=7504'
6: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=selene -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --allred
6: uce_post_accumulation --allreduce_post_accumulation_fp16 --seed=7504
7: STARTING TIMING RUN AT 2021-05-18 09:53:01 PM
0: STARTING TIMING RUN AT 2021-05-18 09:53:01 PM
7: + eval '     ./bind.sh --cpu=exclusive --ib=single --cluster=selene --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=56     --learning_rate=3.5e-4     --opt_lamb_beta_1=0.9     --opt_lamb_beta_2=0.999     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=7100     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.01     --max_samples_termination=4500000     --eval_iter_start_samples=150000 --eval_iter_samples=150000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding     --distributed_lamb   --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks
7: =1      --gradient_accumulation_steps=1     --log_freq=0     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.json --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --seed=7504'
0: + eval '     ./bind.sh --cpu=exclusive --ib=single --cluster=selene --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=56     --learning_rate=3.5e-4     --opt_lamb_beta_1=0.9     --opt_lamb_beta_2=0.999     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=7100     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.01     --max_samples_termination=4500000     --eval_iter_start_samples=150000 --eval_iter_samples=150000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding     --distributed_lamb   --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks
0: =1      --gradient_accumulation_steps=1     --log_freq=0     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.json --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --seed=7504'
7: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=selene -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --allred
7: uce_post_accumulation --allreduce_post_accumulation_fp16 --seed=7504
0: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=selene -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --allred
0: uce_post_accumulation --allreduce_post_accumulation_fp16 --seed=7504
2: STARTING TIMING RUN AT 2021-05-18 09:53:01 PM
5: STARTING TIMING RUN AT 2021-05-18 09:53:01 PM
4: STARTING TIMING RUN AT 2021-05-18 09:53:01 PM
5: + eval '     ./bind.sh --cpu=exclusive --ib=single --cluster=selene --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=56     --learning_rate=3.5e-4     --opt_lamb_beta_1=0.9     --opt_lamb_beta_2=0.999     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=7100     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.01     --max_samples_termination=4500000     --eval_iter_start_samples=150000 --eval_iter_samples=150000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding     --distributed_lamb   --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks
5: =1      --gradient_accumulation_steps=1     --log_freq=0     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.json --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --seed=7504'
2: + eval '     ./bind.sh --cpu=exclusive --ib=single --cluster=selene --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=56     --learning_rate=3.5e-4     --opt_lamb_beta_1=0.9     --opt_lamb_beta_2=0.999     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=7100     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.01     --max_samples_termination=4500000     --eval_iter_start_samples=150000 --eval_iter_samples=150000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding     --distributed_lamb   --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks
2: =1      --gradient_accumulation_steps=1     --log_freq=0     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.json --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --seed=7504'
1: STARTING TIMING RUN AT 2021-05-18 09:53:01 PM
4: + eval '     ./bind.sh --cpu=exclusive --ib=single --cluster=selene --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=56     --learning_rate=3.5e-4     --opt_lamb_beta_1=0.9     --opt_lamb_beta_2=0.999     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=7100     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.01     --max_samples_termination=4500000     --eval_iter_start_samples=150000 --eval_iter_samples=150000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding     --distributed_lamb   --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks
4: =1      --gradient_accumulation_steps=1     --log_freq=0     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.json --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --seed=7504'
5: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=selene -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --allred
5: uce_post_accumulation --allreduce_post_accumulation_fp16 --seed=7504
1: + eval '     ./bind.sh --cpu=exclusive --ib=single --cluster=selene --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=56     --learning_rate=3.5e-4     --opt_lamb_beta_1=0.9     --opt_lamb_beta_2=0.999     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=7100     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.01     --max_samples_termination=4500000     --eval_iter_start_samples=150000 --eval_iter_samples=150000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding     --distributed_lamb   --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks
1: =1      --gradient_accumulation_steps=1     --log_freq=0     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.json --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --seed=7504'
2: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=selene -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --allred
2: uce_post_accumulation --allreduce_post_accumulation_fp16 --seed=7504
4: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=selene -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --allred
4: uce_post_accumulation --allreduce_post_accumulation_fp16 --seed=7504
1: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=selene -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --allred
1: uce_post_accumulation --allreduce_post_accumulation_fp16 --seed=7504
3: num_sockets = 2 num_nodes=8 cores_per_socket=64
3: + exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --allreduce
3: _post_accumulation --allreduce_post_accumulation_fp16 --seed=7504
6: num_sockets = 2 num_nodes=8 cores_per_socket=64
6: + exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --allreduc
6: e_post_accumulation --allreduce_post_accumulation_fp16 --seed=7504
7: num_sockets = 2 num_nodes=8 cores_per_socket=64
7: + exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --allredu
7: ce_post_accumulation --allreduce_post_accumulation_fp16 --seed=7504
1: num_sockets = 2 num_nodes=8 cores_per_socket=64
1: + exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --allreduce
1: _post_accumulation --allreduce_post_accumulation_fp16 --seed=7504
2: num_sockets = 2 num_nodes=8 cores_per_socket=64
4: num_sockets = 2 num_nodes=8 cores_per_socket=64
0: num_sockets = 2 num_nodes=8 cores_per_socket=64
5: num_sockets = 2 num_nodes=8 cores_per_socket=64
2: + exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --allreduce
2: _post_accumulation --allreduce_post_accumulation_fp16 --seed=7504
4: + exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --allreduce
4: _post_accumulation --allreduce_post_accumulation_fp16 --seed=7504
0: + exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --allreduce_
0: post_accumulation --allreduce_post_accumulation_fp16 --seed=7504
5: + exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --allreduce
5: _post_accumulation --allreduce_post_accumulation_fp16 --seed=7504
3: :::MLLOG {"namespace": "", "time_ms": 1621399983029, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 996}}
6: :::MLLOG {"namespace": "", "time_ms": 1621399983095, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 996}}
4: :::MLLOG {"namespace": "", "time_ms": 1621399983181, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 996}}
1: :::MLLOG {"namespace": "", "time_ms": 1621399983224, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 996}}
5: :::MLLOG {"namespace": "", "time_ms": 1621399983258, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 996}}
7: :::MLLOG {"namespace": "", "time_ms": 1621399983303, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 996}}
0: :::MLLOG {"namespace": "", "time_ms": 1621399983325, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 996}}
2: :::MLLOG {"namespace": "", "time_ms": 1621399983334, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 996}}
0: device: cuda:0 n_gpu: 8, distributed training: True, 16-bits training: True
0: :::MLLOG {"namespace": "", "time_ms": 1621399984396, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "bert", "metadata": {"file": "/workspace/bert/mlperf_logger.py", "lineno": 66}}
0: :::MLLOG {"namespace": "", "time_ms": 1621399984396, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/bert/mlperf_logger.py", "lineno": 71}}
0: :::MLLOG {"namespace": "", "time_ms": 1621399984396, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/bert/mlperf_logger.py", "lineno": 75}}
0: :::MLLOG {"namespace": "", "time_ms": 1621399984396, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/bert/mlperf_logger.py", "lineno": 79}}
0: :::MLLOG {"namespace": "", "time_ms": 1621399984396, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xNVIDIA DGX A100", "metadata": {"file": "/workspace/bert/mlperf_logger.py", "lineno": 83}}
0: :::MLLOG {"namespace": "", "time_ms": 1621399984396, "event_type": "POINT_IN_TIME", "key": "seed", "value": 7504, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1006}}
0: :::MLLOG {"namespace": "", "time_ms": 1621399984396, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 448, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1008}}
0: :::MLLOG {"namespace": "", "time_ms": 1621399984396, "event_type": "POINT_IN_TIME", "key": "d_batch_size", "value": 56, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1010}}
0: :::MLLOG {"namespace": "", "time_ms": 1621399984396, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1012}}
7: device: cuda:7 n_gpu: 8, distributed training: True, 16-bits training: True
0: :::MLLOG {"namespace": "", "time_ms": 1621399984396, "event_type": "POINT_IN_TIME", "key": "max_predictions_per_seq", "value": 76, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1014}}
0: :::MLLOG {"namespace": "", "time_ms": 1621399984397, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_training_steps", "value": 7100.0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1016}}
0: :::MLLOG {"namespace": "", "time_ms": 1621399984397, "event_type": "POINT_IN_TIME", "key": "num_warmup_steps", "value": 0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1018}}
0: parsed args:
0: Namespace(allreduce_post_accumulation=True, allreduce_post_accumulation_fp16=True, bert_config_path='/workspace/phase1/bert_config.json', bert_model='bert-large-uncased', bypass_amp=False, cache_eval_data=True, checkpoint_activations=False, cuda_graph_mode='segmented', ddp_type='apex', dense_seq_output=True, device=device(type='cuda', index=0), disable_apex_softmax=False, disable_fuse_mask=False, disable_fuse_qkv=False, disable_fuse_scale=False, distributed_lamb=True, do_train=True, dwu_e5m2_allgather=False, dwu_group_size=0, dwu_num_ag_pg=2, dwu_num_ar_pg=1, dwu_num_blocks=1, dwu_num_chunks=1, dwu_num_rs_pg=1, dwu_overlap_reductions=False, enable_fuse_dropout=False, enable_stream=False, eval_batch_size=16, eval_dir='/workspace/evaldata', eval_iter_samples=150000, eval_iter_start_samples=150000, exchange_padding=True, fp16=True, fused_dropout_add=False, fused_gelu_bias=True, fused_mha=True, gradient_accumulation_steps=1, init_checkpoint='/workspace/phase1/model.ckpt-28252.pt', init_tf_checkpoint=None, input_d
0: ir='/workspace/data_phase2', keep_n_most_recent_checkpoints=20, learning_rate=0.00035, local_rank=0, log_freq=0.0, loss_scale=0.0, max_iterations_per_graph=4, max_predictions_per_seq=76, max_samples_termination=4500000.0, max_seq_length=512, max_steps=7100.0, min_samples_to_start_checkpoints=3000000, n_gpu=8, num_epochs_to_generate_seeds_for=2, num_eval_examples=10000, num_samples_per_checkpoint=500000, opt_lamb_beta_1=0.9, opt_lamb_beta_2=0.999, output_dir='/results', pad=False, phase2=True, resume_from_checkpoint=False, seed=7504, skip_checkpoint=True, start_warmup_step=0.0, target_mlm_accuracy=0.72, train_batch_size=56, train_mlm_accuracy_window_size=0, unpad=True, unpad_fmha=True, use_cuda_graph=False, use_ddp=False, use_env=False, use_gradient_as_bucket_view=False, warmup_proportion=0.0, warmup_steps=0.0, weight_decay_rate=0.01)
5: device: cuda:5 n_gpu: 8, distributed training: True, 16-bits training: True
2: device: cuda:2 n_gpu: 8, distributed training: True, 16-bits training: True
4: device: cuda:4 n_gpu: 8, distributed training: True, 16-bits training: True
1: device: cuda:1 n_gpu: 8, distributed training: True, 16-bits training: True
6: device: cuda:6 n_gpu: 8, distributed training: True, 16-bits training: True
3: device: cuda:3 n_gpu: 8, distributed training: True, 16-bits training: True
0: :::MLLOG {"namespace": "", "time_ms": 1621399990412, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.00035, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 669}}
0: :::MLLOG {"namespace": "", "time_ms": 1621399990462, "event_type": "POINT_IN_TIME", "key": "opt_epsilon", "value": 1e-06, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 699}}
0: :::MLLOG {"namespace": "", "time_ms": 1621399990462, "event_type": "POINT_IN_TIME", "key": "opt_lamb_beta_1", "value": 0.9, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 702}}
0: :::MLLOG {"namespace": "", "time_ms": 1621399990462, "event_type": "POINT_IN_TIME", "key": "opt_lamb_beta_2", "value": 0.999, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 703}}
0: :::MLLOG {"namespace": "", "time_ms": 1621399990462, "event_type": "POINT_IN_TIME", "key": "opt_lamb_weight_decay_rate", "value": 0.0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 704}}
0: :::MLLOG {"namespace": "", "time_ms": 1621399990465, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 86}}
0: :::MLLOG {"namespace": "", "time_ms": 1621399990466, "event_type": "POINT_IN_TIME", "key": "opt_lamb_learning_rate_decay_poly_power", "value": 1.0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 87}}
0: :::MLLOG {"namespace": "", "time_ms": 1621399990466, "event_type": "POINT_IN_TIME", "key": "start_warmup_step", "value": 0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 88}}
5: Torch distributed is available.
5: Torch distributed is initialized.
1: Torch distributed is available.
1: Torch distributed is initialized.
4: Torch distributed is available.
4: Torch distributed is initialized.
3: Torch distributed is available.
3: Torch distributed is initialized.
0: Torch distributed is available.
0: Torch distributed is initialized.
2: Torch distributed is available.
2: Torch distributed is initialized.
6: Torch distributed is available.
6: Torch distributed is initialized.
7: Torch distributed is available.
7: Torch distributed is initialized.
0: :::MLLOG {"namespace": "", "time_ms": 1621400011494, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1264}}
0: :::MLLOG {"namespace": "", "time_ms": 1621400011561, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1265}}
0: :::MLLOG {"namespace": "", "time_ms": 1621400011581, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1276, "epoch_num": 1}}
0: :::MLLOG {"namespace": "", "time_ms": 1621400011582, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1278, "first_epoch_num": 1, "epoch_count": 1}}
0: parsed args:
0: Namespace(allreduce_post_accumulation=True, allreduce_post_accumulation_fp16=True, bert_config_path='/workspace/phase1/bert_config.json', bert_model='bert-large-uncased', bypass_amp=False, cache_eval_data=True, checkpoint_activations=False, cuda_graph_mode='segmented', ddp_type='apex', dense_seq_output=True, device=device(type='cuda', index=0), disable_apex_softmax=False, disable_fuse_mask=False, disable_fuse_qkv=False, disable_fuse_scale=False, distributed_lamb=True, do_train=True, dwu_e5m2_allgather=False, dwu_group_size=0, dwu_num_ag_pg=2, dwu_num_ar_pg=1, dwu_num_blocks=1, dwu_num_chunks=1, dwu_num_rs_pg=1, dwu_overlap_reductions=False, enable_fuse_dropout=False, enable_stream=False, eval_batch_size=16, eval_dir='/workspace/evaldata', eval_iter_samples=150000, eval_iter_start_samples=150000, exchange_padding=True, fp16=True, fused_dropout_add=False, fused_gelu_bias=True, fused_mha=True, gradient_accumulation_steps=1, init_checkpoint='/workspace/phase1/model.ckpt-28252.pt', init_tf_checkpoint=None, input_d
0: ir='/workspace/data_phase2', keep_n_most_recent_checkpoints=20, learning_rate=0.00035, local_rank=0, log_freq=0.0, loss_scale=0.0, max_iterations_per_graph=4, max_predictions_per_seq=76, max_samples_termination=4500000.0, max_seq_length=512, max_steps=7100.0, min_samples_to_start_checkpoints=3000000, n_gpu=8, num_epochs_to_generate_seeds_for=2, num_eval_examples=10000, num_samples_per_checkpoint=500000, opt_lamb_beta_1=0.9, opt_lamb_beta_2=0.999, output_dir='/results', pad=False, phase2=True, resume_from_checkpoint=False, resume_step=0, seed=7504, skip_checkpoint=True, start_warmup_step=0.0, target_mlm_accuracy=0.72, train_batch_size=56, train_mlm_accuracy_window_size=0, unpad=True, unpad_fmha=True, use_cuda_graph=False, use_ddp=False, use_env=False, use_gradient_as_bucket_view=False, warmup_proportion=0.0, warmup_steps=0.0, weight_decay_rate=0.01)
0: epoch: 1
0: :::MLLOG {"namespace": "", "time_ms": 1621400077768, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.3736896812915802, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 335, 'eval_loss': 4.111658096313477, 'eval_mlm_accuracy': 0.3736896812915802}
0: :::MLLOG {"namespace": "", "time_ms": 1621400141717, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.398612916469574, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 670, 'eval_loss': 3.896031618118286, 'eval_mlm_accuracy': 0.398612916469574}
0: :::MLLOG {"namespace": "", "time_ms": 1621400214471, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.4297126531600952, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 1005, 'eval_loss': 3.6116104125976562, 'eval_mlm_accuracy': 0.4297126531600952}
0: :::MLLOG {"namespace": "", "time_ms": 1621400287056, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.48250141739845276, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 1340, 'eval_loss': 3.1574339866638184, 'eval_mlm_accuracy': 0.48250141739845276}
0: :::MLLOG {"namespace": "", "time_ms": 1621400357661, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.5606547594070435, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 1675, 'eval_loss': 2.5010221004486084, 'eval_mlm_accuracy': 0.5606547594070435}
0: :::MLLOG {"namespace": "", "time_ms": 1621400427702, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.6516048312187195, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 2009, 'eval_loss': 1.7903474569320679, 'eval_mlm_accuracy': 0.6516048312187195}
0: :::MLLOG {"namespace": "", "time_ms": 1621400493941, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7007741332054138, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 2344, 'eval_loss': 1.431792140007019, 'eval_mlm_accuracy': 0.7007741332054138}
0: :::MLLOG {"namespace": "", "time_ms": 1621400559177, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7073265910148621, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 2679, 'eval_loss': 1.386763095855713, 'eval_mlm_accuracy': 0.7073265910148621}
0: :::MLLOG {"namespace": "", "time_ms": 1621400625519, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7103132605552673, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 3014, 'eval_loss': 1.36533522605896, 'eval_mlm_accuracy': 0.7103132605552673}
0: :::MLLOG {"namespace": "", "time_ms": 1621400691980, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7123472094535828, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 3349, 'eval_loss': 1.3588852882385254, 'eval_mlm_accuracy': 0.7123472094535828}
0: :::MLLOG {"namespace": "", "time_ms": 1621400755855, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7133349776268005, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 3684, 'eval_loss': 1.3479962348937988, 'eval_mlm_accuracy': 0.7133349776268005}
0: :::MLLOG {"namespace": "", "time_ms": 1621400818810, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7142736911773682, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 4018, 'eval_loss': 1.342798113822937, 'eval_mlm_accuracy': 0.7142736911773682}
0: :::MLLOG {"namespace": "", "time_ms": 1621400892548, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.71612548828125, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 4353, 'eval_loss': 1.3343896865844727, 'eval_mlm_accuracy': 0.71612548828125}
0: :::MLLOG {"namespace": "", "time_ms": 1621400969135, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7167302966117859, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 4688, 'eval_loss': 1.3265734910964966, 'eval_mlm_accuracy': 0.7167302966117859}
0: :::MLLOG {"namespace": "", "time_ms": 1621401039352, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7173888087272644, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 5023, 'eval_loss': 1.3216395378112793, 'eval_mlm_accuracy': 0.7173888087272644}
0: :::MLLOG {"namespace": "", "time_ms": 1621401107137, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7177414298057556, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 5358, 'eval_loss': 1.320426106452942, 'eval_mlm_accuracy': 0.7177414298057556}
0: :::MLLOG {"namespace": "", "time_ms": 1621401176788, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7187502384185791, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 5692, 'eval_loss': 1.3155580759048462, 'eval_mlm_accuracy': 0.7187502384185791}
0: :::MLLOG {"namespace": "", "time_ms": 1621401247440, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7192359566688538, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 6027, 'eval_loss': 1.312723159790039, 'eval_mlm_accuracy': 0.7192359566688538}
0: :::MLLOG {"namespace": "", "time_ms": 1621401318755, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7197473645210266, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 6362, 'eval_loss': 1.3109309673309326, 'eval_mlm_accuracy': 0.7197473645210266}
0: :::MLLOG {"namespace": "", "time_ms": 1621401390081, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7200018763542175, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 6697, 'eval_loss': 1.3089025020599365, 'eval_mlm_accuracy': 0.7200018763542175}
0: 0.720002 > 0.720000, Target MLM Accuracy reached at 6697
0: (1, 6708.0) {'final_loss': 0.0}
0: :::MLLOG {"namespace": "", "time_ms": 1621401390141, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1567, "first_epoch_num": 1}}
0: :::MLLOG {"namespace": "", "time_ms": 1621401390141, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1570, "epoch_num": 1}}
0: :::MLLOG {"namespace": "", "time_ms": 1621401390141, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3000256, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1574}}
0: :::MLLOG {"namespace": "", "time_ms": 1621401390141, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 10000, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1577}}
0: :::MLLOG {"namespace": "", "time_ms": 1621401390141, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1580, "status": "success"}}
0: {'e2e_time': 1406.8784852027893, 'training_sequences_per_second': 2280.5468452821874, 'final_loss': 0.0, 'raw_train_time': 1394.7531955242157}
7: ++ date +%s
7: + END=1621401392
7: ++ date '+%Y-%m-%d %r'
7: + END_FMT='2021-05-18 10:16:32 PM'
7: + echo 'ENDING TIMING RUN AT 2021-05-18 10:16:32 PM'
7: ENDING TIMING RUN AT 2021-05-18 10:16:32 PM
7: + RESULT=1411
7: + RESULT_NAME=bert
7: RESULT,bert,7504,1411,root,2021-05-18 09:53:01 PM
7: + echo 'RESULT,bert,7504,1411,root,2021-05-18 09:53:01 PM'
7: + set +x
0: ++ date +%s
0: + END=1621401392
0: ++ date '+%Y-%m-%d %r'
0: + END_FMT='2021-05-18 10:16:32 PM'
0: + echo 'ENDING TIMING RUN AT 2021-05-18 10:16:32 PM'
0: ENDING TIMING RUN AT 2021-05-18 10:16:32 PM
0: + RESULT=1411
0: + RESULT_NAME=bert
0: RESULT,bert,7504,1411,root,2021-05-18 09:53:01 PM
0: + echo 'RESULT,bert,7504,1411,root,2021-05-18 09:53:01 PM'
0: + set +x
6: ++ date +%s
6: + END=1621401392
6: ++ date '+%Y-%m-%d %r'
6: + END_FMT='2021-05-18 10:16:32 PM'
6: + echo 'ENDING TIMING RUN AT 2021-05-18 10:16:32 PM'
6: ENDING TIMING RUN AT 2021-05-18 10:16:32 PM
6: + RESULT=1411
6: + RESULT_NAME=bert
6: RESULT,bert,7504,1411,root,2021-05-18 09:53:01 PM
6: + echo 'RESULT,bert,7504,1411,root,2021-05-18 09:53:01 PM'
6: + set +x
3: ++ date +%s
3: + END=1621401393
3: ++ date '+%Y-%m-%d %r'
4: ++ date +%s
5: ++ date +%s
2: ++ date +%s
3: + END_FMT='2021-05-18 10:16:33 PM'
3: ENDING TIMING RUN AT 2021-05-18 10:16:33 PM
3: + echo 'ENDING TIMING RUN AT 2021-05-18 10:16:33 PM'
3: + RESULT=1412
3: + RESULT_NAME=bert
3: RESULT,bert,7504,1412,root,2021-05-18 09:53:01 PM
3: + echo 'RESULT,bert,7504,1412,root,2021-05-18 09:53:01 PM'
3: + set +x
5: + END=1621401393
4: + END=1621401393
2: + END=1621401393
5: ++ date '+%Y-%m-%d %r'
4: ++ date '+%Y-%m-%d %r'
2: ++ date '+%Y-%m-%d %r'
5: + END_FMT='2021-05-18 10:16:33 PM'
5: + echo 'ENDING TIMING RUN AT 2021-05-18 10:16:33 PM'
5: ENDING TIMING RUN AT 2021-05-18 10:16:33 PM
5: + RESULT=1412
5: + RESULT_NAME=bert
5: RESULT,bert,7504,1412,root,2021-05-18 09:53:01 PM
5: + echo 'RESULT,bert,7504,1412,root,2021-05-18 09:53:01 PM'
5: + set +x
4: + END_FMT='2021-05-18 10:16:33 PM'
4: ENDING TIMING RUN AT 2021-05-18 10:16:33 PM
4: + echo 'ENDING TIMING RUN AT 2021-05-18 10:16:33 PM'
4: + RESULT=1412
4: + RESULT_NAME=bert
2: + END_FMT='2021-05-18 10:16:33 PM'
4: RESULT,bert,7504,1412,root,2021-05-18 09:53:01 PM
4: + echo 'RESULT,bert,7504,1412,root,2021-05-18 09:53:01 PM'
4: + set +x
2: + echo 'ENDING TIMING RUN AT 2021-05-18 10:16:33 PM'
2: ENDING TIMING RUN AT 2021-05-18 10:16:33 PM
2: + RESULT=1412
2: + RESULT_NAME=bert
2: RESULT,bert,7504,1412,root,2021-05-18 09:53:01 PM
2: + echo 'RESULT,bert,7504,1412,root,2021-05-18 09:53:01 PM'
2: + set +x
1: ++ date +%s
1: + END=1621401393
1: ++ date '+%Y-%m-%d %r'
1: + END_FMT='2021-05-18 10:16:33 PM'
1: + echo 'ENDING TIMING RUN AT 2021-05-18 10:16:33 PM'
1: ENDING TIMING RUN AT 2021-05-18 10:16:33 PM
1: + RESULT=1412
1: + RESULT_NAME=bert
1: RESULT,bert,7504,1412,root,2021-05-18 09:53:01 PM
1: + echo 'RESULT,bert,7504,1412,root,2021-05-18 09:53:01 PM'
1: + set +x
